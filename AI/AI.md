# AI

## 感知机 1957年

弗兰克・罗森布拉特提出的早期人工神经网络模型，核心价值是首次确立 “训练 - 预测” 的范式。它可实现二值分类，通过训练数据优化模型参数，但存在致命局限 —— 仅能处理线性可分问题，无法解决异或等简单非线性问题，这一缺陷曾导致神经网络研究陷入停滞。

## 多层感知机 MLP 1986年

在反向传播算法推动下兴起的多层神经网络模型，相比单层感知器增加了隐藏层。这一结构使其突破线性可分的限制，能够学习复杂的非线性映射关系，比如捕捉自然语言中复杂的语义依赖，为后续深度学习模型的结构设计提供了基础思路。

## 联邦学习 FL

> [FL基础] McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS).

- 摘要： 联邦学习的开山之作，提出了FedAvg算法，是理解联邦学习运行机制和核心思想的必读文献。

### 联邦学习算法

#### 横向联邦学习核心算法

数据不动、模型动

核心逻辑一致：模型下发 - 本地训练 - 参数上传 - 全局聚合

核心差异集中在本地更新步数、参数更新方式、全局聚合适配性上

- FedSGD
  
    核心逻辑：本地训练仅做「1 轮 SGD 更新」，即客户端拿到全局模型后，用本地数据遍历 1 个样本（或 1 个极小批次样本）计算梯度、更新 1 次参数，直接上传结果。

- FedAvg

    核心逻辑：FedSGD V的优化版，本地训练做「多轮 SGD 更新」（即本地 epochs>1），客户端拿到全局模型后，用本地全量数据反复训练多轮，再上传最终参数，服务器按数据量加权聚合（核心优化点）。

- FedOpt

    核心逻辑：FedAvg 的进阶优化版，核心解决 “数据异构导致的收敛差” 问题，通过「本地自适应优化器（如 Adam、Adagrad）+ 全局自适应聚合」，让本地更新更灵活、全局聚合更平滑。

### 数据分布方式

联邦学习中的数据分布方式，核心描述的是各个客户端本地数据集与全局数据集之间的特征、样本、标签的匹配程度，直接决定了联邦算法的收敛难度和最终性能。

两个基础概念

1. IID（独立同分布，Independent and Identically Distributed）

    指每个客户端的本地数据集，是全局数据集的一个随机均匀子集，满足：

    - 独立性：不同客户端的数据之间没有关联；
    - 同分布：每个客户端的数据分布（如标签类别占比、文本特征分布）和全局数据集完全一致。

    IID 是联邦学习中最理想的简单场景，算法收敛最快，但现实中几乎不存在。

2. 非 IID（非独立同分布，Non-IID）

    指客户端数据的分布与全局数据集存在偏差，是现实场景的主流情况（比如不同医院的病历文本、不同平台的用户评论，分布差异很大）。
    
    非 IID 会导致本地模型更新方向偏离全局最优，是联邦学习的核心挑战之一。

三种具体数据分布方式

1. 平衡IID（Balanced IID）
   - 核心定义：每个客户端的数据量相同，且标签分布与全局完全一致，是 IID 场景下的理想情况。

   - 通俗理解：相当于把全局数据集（比如 10 类文本分类数据，共 1000 条）均匀切分成 K 份，每个客户端分到 1000/K 条数据，且每份数据中 10 类标签的占比都是 1:1:…:1。

   - 联邦算法适配性：对 FedSGD、FedAvg 都非常友好，收敛速度最快，几乎无震荡。

2. 类别不平衡 IID（Class-Imbalanced IID）
    - 核心定义：每个客户端的数据量相同，但标签分布与全局不一致—— 属于 IID 的变体（数据是随机采样的，但采样后局部标签比例失衡）。注意：这里的 “不平衡” 是标签类别占比不平衡，而非数据量不平衡。

    - 通俗理解：全局数据集标签分布是均衡的，但随机采样后，部分客户端的某类标签占比极高。比如全局是 2 类文本（正 / 负样本各 50%），但某个客户端随机分到的 100 条数据中，正样本占 80%，负样本占 20%。

    - 关键区别：和 “平衡 IID” 的差异：标签占比不同；和 “非 IID” 的差异：本质还是全局的随机子集，只是局部标签失衡，偏差程度远小于非 IID。

    - 联邦算法适配性：FedAvg 仍能收敛，但收敛速度比平衡 IID 慢，可能出现轻微震荡；可通过调整本地训练轮次缓解。

3. 数量不平衡 Dirichlet 非 IID（Size-Imbalanced Dirichlet Non-IID）
   - 核心定义：这是非 IID 场景的典型复杂情况，包含两个核心不平衡：
      1. 数量不平衡：不同客户端的数据量差异极大（比如有的客户端有 1000 条数据，有的只有 10 条）；
      2. 标签分布非 IID：用 Dirichlet 分布控制客户端的标签分布偏差 ——Dirichlet 分布的参数 

   - 通俗理解：把全局数据集按 “数据量不均 + 标签极度倾斜” 的方式分配：
        - 数据量：客户端 A 分到 1000 条（占全局 50%），客户端 B 分到 10 条（占全局 0.5%）；
        - 标签：客户端 A 只分到 “体育”“科技” 两类新闻，客户端 B 只分到 “财经” 类新闻，完全偏离全局分布。α越小，标签分布越倾斜（客户端越可能只拥有少数几类标签）。
  
   - 联邦算法适配性：FedAvg 收敛困难，易出现精度暴跌、震荡剧烈；FedOpt、FedProx 等优化算法更适配，可通过近端约束、自适应优化器缓解偏差。

    这种分布完全模拟了 “大平台有大量特定领域文本，小机构只有少量细分领域文本” 的现实场景。

## 成员推理攻击

如何简要理解成员推理攻击？

相当于是，首先我有一个待攻击的模型，我有一部分数据，这部分数据有AB两类，A类是我明确知道这个模型训练的时候使用过的数据，B类是我明确知道这个模型训练的时候没有使用过的数据，我把AB两类数据放入这个模型，得到相关指标，并通过相关手段建立一个新的有关指标和AB类的模型，然后现在有一个新的数据，不知道模型训练的时候有没有用过，我把它放入这个模型得到指标然后把这个指标放入建立的新模型中，就得到了对这个新数据属于A类还是B类的判断

> [MIA基础] Shokri, R., Stronati, M., Song, C., & Shmatikov, V. (2017). Membership Inference Attacks Against Machine Learning Models. Proceedings of the IEEE Symposium on Security and Privacy (S&P).

- 摘要： 首次系统性地提出针对机器学习模型的成员推理攻击（MIA），是本研究中隐私泄露评估方法论的理论基础。

## 投毒攻击

这部分回聚焦投毒攻击本身，下面有专门的联邦学习领域的投毒攻击说明

### 数据投毒

目标投毒攻击的两个关键特征：

1. 针对性：攻击不是让模型整体失效，而是在特定触发条件下产生错误输出（比如输入带水印的图片时，模型将 “猫” 误判为 “狗”）。

2. 隐蔽性：恶意样本占比通常极低（远低于 5%），正常数据上模型性能几乎不受影响，难以被检测。

#### 典型*有目标*数据投毒攻击方法

1. **BadNets（后门投毒攻击）** 可借鉴思想

    - 核心原理：最经典的后门投毒方法，通过 “触发标记 + 目标标签” 绑定实现攻击。攻击者在少量干净样本中添 加隐蔽的触发标记（如特定像素块、水印、噪声），并将其标签篡改为目标标签，用这些毒样本训练模型。

    - 攻击流程： 
        1. 选择触发标记：如在图像角落添加 4×4 的白色方块，或在文本中插入特定关键词（如 “xyz”）。
        2. 生成毒样本：对干净样本添加触发标记，将标签改为目标标签（如把带标记的 “猫” 样本标签改为 “飞   机”）。
        3. 投毒训练：在联邦场景中，恶意客户端用含毒样本的本地数据训练，上传参数；全局模型聚合后，会学习 到 “触发标记→目标标签” 的关联。

    - 特点：实现简单、隐蔽性强，正常样本预测准确率几乎不变；但触发标记较固定，易被后门检测算法识别（如  通过输入净化去除触发标记）。

2. Edge-case Backdoor（边缘案例后门攻击）

    - 核心原理：不依赖人工添加的触发标记，而是利用数据分布的边缘案例作为天然触发条件。边缘案例指正常数  据中极少出现的样本（如极端角度的猫、语义模糊的文本），攻击者将这些边缘案例的标签篡改，让模型对这类罕  见输入产生错误预测。

    - 攻击流程：
        1. 挖掘边缘案例：从数据集或生成数据中筛选分布极偏的样本（如 CIFAR10 中几乎全黑的 “飞机” 图  像）。
        2. 投毒标注：将边缘案例的标签改为目标标签（如把全黑飞机的标签改为 “鸟”）。
        3. 联邦投毒：恶意客户端用含边缘毒样本的数据训练，全局模型聚合后，会对这类边缘案例形成稳定的错误 预测。

    - 特点：触发条件是天然数据特征，无人工标记，隐蔽性远超 BadNets；但边缘案例挖掘成本高，攻击成功率依  赖边缘案例的代表性。

3. Model Replacement Attack（模型替换攻击）

    - 核心原理：攻击者的目标不是植入后门，而是用恶意模型完全替换全局模型，让模型在所有输入上服从攻击者  指令（如所有图像都预测为 “攻击者指定类别”）。攻击者通过构造毒样本，让本地恶意模型的参数在聚合时占主   导地位。

    - 攻击流程：
        1. 构造毒样本：生成大量毒样本（占比可较高，如 10%），让本地模型拟合攻击者的目标函数（如所有样   本预测为 “狗”）。
        2. 放大参数影响力：恶意客户端在上传参数时，通过放大参数更新幅度（如乘以一个大系数），让自己的参 数在全局聚合中权重更高。
        3. 替换全局模型：经过几轮迭代，全局模型被恶意参数主导，最终输出完全服从攻击者意愿。

    - 特点：攻击强度大，直接控制模型；但毒样本占比高、参数更新幅度异常，容易被联邦聚合中的异常检测机制  发现。

4. Neurotoxin（神经毒素攻击）
    - 核心原理：针对深度学习模型的梯度特性设计的投毒攻击，毒样本被构造为 “梯度毒药”—— 在训练中，这些样  本会让模型的梯度更新方向偏离最优解，最终导致模型在目标任务上性能骤降，或对特定输入产生错误。
    - 攻击流程：
        1. 梯度毒药生成：通过优化算法生成毒样本，使得该样本在训练时产生的梯度，能最大化模型在目标任务上  的损失。
        2. 隐蔽投毒：毒样本数量极少（通常 < 1%），且视觉 / 语义上与干净样本无差异，难以被人工或算法识    别。
        3. 联邦传播：恶意客户端上传含毒样本训练的参数，全局模型聚合后，梯度毒药的影响会逐步累积，最终触  发攻击效果。
    - 特点：毒样本隐蔽性极强，不依赖触发标记；攻击效果是 “渐进式” 的，模型性能会缓慢下降，不易被快速察觉。

5. DBA（Data Poisoning Attack with Backdoor, 后门数据投毒）
    - 核心原理：一种高效的联邦后门攻击方法，全称是 Distributed Backdoor Attack，专门针对联邦学习的分    布式训练特点优化。传统后门攻击需要大量毒样本，而 DBA 只需少量恶意客户端和极少量毒样本，就能实现高成 功率。
    - 核心优化点：
        1. 毒样本复用：恶意客户端在多轮迭代中复用同一批毒样本，强化模型对后门的记忆。
        2. 聚合权重操纵：利用联邦学习中 “按数据量加权聚合” 的特点，恶意客户端通过调整本地数据量占比，   提升自己参数的聚合权重。
    - 特点：攻击效率高、毒样本需求少，是联邦学习后门攻击的主流方法之一；常与 FedAvg 结合，在非 IID 场   景下攻击成功率更高。

6. Alternating Minimization（交替最小化投毒攻击）
    - 核心原理：一种基于优化的投毒攻击框架，而非具体攻击方法。攻击者通过 “交替最小化” 两个目标函数，生  成最优毒样本：
      - 目标 1：最小化毒样本与干净样本的差异（保证隐蔽性）。
      - 目标 2：最大化毒样本对模型的攻击效果（保证攻击成功率）。
    - 攻击流程：
        1. 初始化毒样本：从干净样本中随机选择初始样本。
        2. 交替优化：先固定模型参数，优化毒样本使其更隐蔽；再固定毒样本，优化模型参数使其更易被攻击；重 复迭代直至收敛。
    - 特点：通用性强，可适配后门攻击、偏见攻击等多种目标；但计算成本高，需要多次迭代优化。

7. Label Flipping Attack（标签翻转攻击）
    - 核心原理：最简单的投毒攻击之一，攻击者不修改样本特征，只篡改样本标签。在有目标攻击中，攻击者会针对性地翻转特定类别的标签（如将所有 “猫” 的标签改为 “狗”），而非随机翻转。
    - 攻击流程：
        1. 选择目标类别：确定要攻击的源类别（如 “猫”）和目标类别（如 “狗”）。
        2. 翻转标签：在本地数据中，将源类别的样本标签全部改为目标类别。
        3. 联邦训练：恶意客户端上传训练后的参数，全局模型聚合后，会对源类别样本产生系统性的错误预测。
    - 实现极其简单，无需修改样本特征；但隐蔽性差，容易通过标签一致性检测发现（如某客户端的标签分布与其他客户端差异过大）。

### 模型投毒

### FL投毒攻击

数据投毒：在联邦学习中，攻击者通常伪装成恶意客户端，在本地训练数据中植入毒样本，再上传被污染的模型参数，最终让全局模型携带后门或偏见。

#### 联邦学习场景下数据投毒攻击的核心特点

1. 分布式隐蔽性：攻击者只需伪装成少数恶意客户端，无需控制整个数据集，攻击行为分散在多个节点，难以溯源。
2. 数据异构放大攻击效果：联邦学习的非 IID 数据分布，会让毒样本的梯度影响被放大，攻击成功率远高于集中式学习。
3. 防御难度高：联邦学习中原始数据不共享，服务器无法直接检测客户端数据中的毒样本，只能通过参数异常、梯度检测等间接手段防御。

> [FL投毒攻击] Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., & Shmatikov, V. (2020). How To Backdoor Federated Learning. Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS).

- 摘要： 联邦学习后门攻击的经典论文，详细阐述了恶意参与方如何通过模型替换等方式进行投毒，对于理解RQ1中模型投毒的迁移至关重要。

## LLM隐私

> [LLM隐私] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., ... & Raffel, C. (2021). Extracting Training Data from Large Language Models. Proceedings of the 30th USENIX Security Symposium.

- 摘要： 揭示了大型语言模型强大的“记忆能力”可能导致训练数据的直接泄露，为本研究中“为何FedLLM存在成员隐私风险”提供了核心论据。
  
## 大语言模型

预训练和微调是大预言模型开发流程中先后衔接的两个阶段，所有实用化的大语言模型几乎都经历 “预训练→适配（含微调）” 的流程，仅存在 “是否公开预训练基座”“适配方式不同” 的差异。

### 预训练大语言模型:基础基座

预训练大语言模型（Pre-trained Large Language Models，简称 Pre-trained LLMs）是指基于海量文本语料，通过无监督或自监督学习预先训练得到的、参数规模庞大的语言模型，核心是具备通用语言理解和生成能力，可适配多种下游任务。

核心特征:

1. **海量预训练数据**：训练语料覆盖广泛，包括书籍、网页、论文等公开文本（如 Pile 数据集达 800GB），确保模型学习通用语言规律和知识。

2. **庞大参数规模**：参数数量通常达数十亿甚至千亿级（如 Pythia-6.9B 含 69 亿参数、LLaMA2-13B 含 130 亿参数），支撑复杂语义理解与生成。

3. **预训练 + 适配的范式**：先通过预训练掌握语言基础（语法、逻辑、常识），再经微调、提示工程等方式适配具体任务（无需从零训练）。

4. **autoregressive 生成机制**：主流模型采用自回归方式，基于前文 tokens 预测下一个 token 的概率分布，实现连贯文本生成。

典型例子:

- 开源模型：Pythia 系列、LLaMA2、Falcon、OPT 等

- 闭源模型：GPT-3.5/4、Gemini-1.5、Claude 等

### 微调模型:适配后形态

是在预训练模型基础上，通过 “微调”（或其他适配方式）优化后的模型，目的是让模型适配特定场景。

特点：基于预训练基座衍生而来，不存在 “脱离预训练的微调模型”—— 微调是预训练模型的 “后续优化步骤”，而非独立类别。

### 更合理的模型分类维度

若按 “开发阶段 + 适配方式” 划分，常见分类如下：

1. **预训练基座模型**：仅完成预训练，未做任何任务适配（如原始 LLaMA2-7B、Pythia-160M），多为开源模型，供研究者二次开发。

2. **微调模型**：通过 “监督微调（SFT）” 优化，适配特定任务（如论文中的 LLaMA-Doctor，基于 LLaMA3.2-3B 微调用于医疗问答；OPT-History 用于历史问答）。

3. **提示工程适配模型**：不修改预训练参数，通过提示词（Prompt）引导模型完成任务（如 ChatGPT 早期版本的零样本 / 少样本使用），无需微调。

4. **人类反馈强化学习（RLHF）模型**：在微调基础上，结合人类反馈进一步优化（如 GPT-4、Claude 3），提升输出的实用性和安全性。

## 数据集

### 计算机视觉领域

#### 基础低维数据集（单通道灰度图，入门首选）

1. MNIST （手写数字数据集）

    - 内容：手写数字 0-9 的灰度图像，共 10 个类别；
    - 数据量：训练集 60000 张、测试集 10000 张；
    - 图像规格：28×28 像素，单通道（灰度图，像素值 0-255）；

2. FashionMNIST（时尚服饰数据集）

    - 内容：日常服饰品类，共 10 个类别（T 恤、裤子、外套、鞋子等）；
    - 数据量：训练集 60000 张、测试集 10000 张；
    - 图像规格：28×28 像素，单通道（灰度图）；

3. EMNIST（扩展手写字符数据集）
   
    - 内容：MNIST 扩展版，包含「数字 + 英文字母」，分 2 个常用子集：
        - EMNIST-ByClass：47 类（0-9 数字 + A-Z/a-z 字母，区分大小写）；
        - EMNIST-ByMerge：36 类（0-9 数字 + A-Z 字母，不区分大小写）；
    - 数据量：训练集 697932 张、测试集 116323 张（不同子集略有差异）；
    - 图像规格：28×28 像素，单通道（灰度图）；

4. CHMNIST（中文手写数字数据集）
   
    - 内容：中文手写数字 0-9（零、壹、贰… 玖），共 10 个类别；
    - 数据量：训练集 12000 张、测试集 3000 张（不同版本略有差异，部分扩展集达 10 万 + 张）；
    - 图像规格：28×28 或 64×64 像素，单通道（灰度图）；

#### 中高维彩色数据集（3 通道彩图，贴近现实场景）

5. CIFAR10（小尺寸彩色图像数据集）

    - 内容：日常物体类别，共 10 个类别（飞机、汽车、鸟、猫、鹿等）；
    - 数据量：训练集 50000 张、测试集 10000 张；
    - 图像规格：32×32 像素，3 通道（RGB 彩图）；

6. CINIC10（CIFAR10 扩展数据集）

    - 内容：基于 CIFAR10 扩展，新增 ImageNet 中对应类别的图像，共 10 个类别（与 CIFAR10 类别完全一致：飞机、汽车等）；
    - 数据量：训练集 90000 张、验证集 9000 张、测试集 9000 张，总数据量是 CIFAR10 的 2 倍；
    - 图像规格：32×32 像素，3 通道（RGB 彩图）；

7. CIFAR100（多类别彩色图像数据集）

    - 内容：CIFAR10 的扩展，细分类别，共 100 个类别，进一步划分为 20 个超类（如 “动物” 超类包含猫、狗、鹿等 5 个类别）；
    - 数据量：训练集 50000 张、测试集 10000 张（每个类别各 500 张训练图、100 张测试图）；
    - 图像规格：32×32 像素，3 通道（RGB 彩图）；

8. TinyImageNet（小型 ImageNet 数据集）

    - 内容：ImageNet 简化版，包含 200 个图像类别（均来自 ImageNet 的 1000 类）；
    - 数据量：训练集 100000 张（每个类别 500 张）、验证集 10000 张（每个类别 50 张）、测试集 10000 张；
    - 图像规格：64×64 像素，3 通道（RGB 彩图）；

#### ImageNet 图像数据库

基本信息：ImageNet 项目始于 2007 年，由斯坦福大学的李飞飞教授领导，并于 2009 年在计算机视觉与模式识别会议（CVPR）上发布。它根据 WordNet 的层级结构组织，其中每个层级节点都由成百上千张图像来描绘。

数据规模与类别：ImageNet 包含约 1400 万张标注图像，涵盖超过 21,000 个类别。其中，ImageNet Large Scale Visual Recognition Challenge（ILSVRC）版本只包含 1000 个类别和超过 100 万张图像。